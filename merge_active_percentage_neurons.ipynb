{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import pickle as pkl\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'c:\\\\Users\\\\ding\\\\Documents\\\\github\\\\CeA'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = r'C:/Users/ding/Documents/github/CeA/data'  \n",
    "file_list = os.listdir(data_dir)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "EmptyDataError",
     "evalue": "No columns to parse from file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mEmptyDataError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [7], line 65\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m merged_data\n\u001b[0;32m     62\u001b[0m \u001b[38;5;66;03m# Define directory and file list\u001b[39;00m\n\u001b[0;32m     63\u001b[0m \n\u001b[0;32m     64\u001b[0m \u001b[38;5;66;03m# Process files and create the merged DataFrame with Z-scores\u001b[39;00m\n\u001b[1;32m---> 65\u001b[0m merged_data_with_zscore \u001b[38;5;241m=\u001b[39m \u001b[43mprocess_files_with_zscore\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfile_list\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;66;03m# Save the merged data with Z-scores to a new CSV file (optional)\u001b[39;00m\n\u001b[0;32m     68\u001b[0m output_path \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/ding/Documents/github/CeA\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaterial_merged_data_with_zscore.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[1;32mIn [7], line 46\u001b[0m, in \u001b[0;36mprocess_files_with_zscore\u001b[1;34m(data_dir, file_list)\u001b[0m\n\u001b[0;32m     42\u001b[0m condition \u001b[38;5;241m=\u001b[39m parts[\u001b[38;5;241m1\u001b[39m]\n\u001b[0;32m     43\u001b[0m \u001b[38;5;66;03m#print(parts[1])\u001b[39;00m\n\u001b[0;32m     44\u001b[0m \n\u001b[0;32m     45\u001b[0m \u001b[38;5;66;03m# Load the file\u001b[39;00m\n\u001b[1;32m---> 46\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     48\u001b[0m \u001b[38;5;66;03m# Restructure the data with Z-scores\u001b[39;00m\n\u001b[0;32m     49\u001b[0m restructured_data \u001b[38;5;241m=\u001b[39m restructure_data_with_zscore(data)\n",
      "File \u001b[1;32mc:\\Users\\ding\\Miniconda3\\envs\\ca\\lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ding\\Miniconda3\\envs\\ca\\lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\ding\\Miniconda3\\envs\\ca\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:950\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, squeeze, prefix, mangle_dupe_cols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, error_bad_lines, warn_bad_lines, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options)\u001b[0m\n\u001b[0;32m    935\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m    936\u001b[0m     dialect,\n\u001b[0;32m    937\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    946\u001b[0m     defaults\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdelimiter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m,\u001b[39m\u001b[38;5;124m\"\u001b[39m},\n\u001b[0;32m    947\u001b[0m )\n\u001b[0;32m    948\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m--> 950\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_read\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilepath_or_buffer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ding\\Miniconda3\\envs\\ca\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:605\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    602\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    604\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 605\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    607\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    608\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32mc:\\Users\\ding\\Miniconda3\\envs\\ca\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1442\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1439\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1441\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1442\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_make_engine\u001b[49m\u001b[43m(\u001b[49m\u001b[43mf\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mengine\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\ding\\Miniconda3\\envs\\ca\\lib\\site-packages\\pandas\\io\\parsers\\readers.py:1753\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1750\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg)\n\u001b[0;32m   1752\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1753\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mapping[engine](f, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions)\n\u001b[0;32m   1754\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[0;32m   1755\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\ding\\Miniconda3\\envs\\ca\\lib\\site-packages\\pandas\\io\\parsers\\c_parser_wrapper.py:79\u001b[0m, in \u001b[0;36mCParserWrapper.__init__\u001b[1;34m(self, src, **kwds)\u001b[0m\n\u001b[0;32m     76\u001b[0m     kwds\u001b[38;5;241m.\u001b[39mpop(key, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[0;32m     78\u001b[0m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m ensure_dtype_objs(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[1;32m---> 79\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader \u001b[38;5;241m=\u001b[39m parsers\u001b[38;5;241m.\u001b[39mTextReader(src, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     81\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39munnamed_cols \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reader\u001b[38;5;241m.\u001b[39munnamed_cols\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# error: Cannot determine type of 'names'\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\ding\\Miniconda3\\envs\\ca\\lib\\site-packages\\pandas\\_libs\\parsers.pyx:554\u001b[0m, in \u001b[0;36mpandas._libs.parsers.TextReader.__cinit__\u001b[1;34m()\u001b[0m\n",
      "\u001b[1;31mEmptyDataError\u001b[0m: No columns to parse from file"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Function to restructure data and calculate Z-scores\n",
    "def restructure_data_with_zscore(data):\n",
    "    \"\"\"\n",
    "    Restructure the data to create a merged table where each column corresponds to a neuron \n",
    "    or an experimental variable and each row corresponds to a time point. Also calculates Z-scores.\n",
    "    \"\"\"\n",
    "    data_group = data.groupby('Cell')\n",
    "    template_df = None\n",
    "    for k in data_group.groups.keys():\n",
    "        if template_df is None:\n",
    "            template_df = data_group.get_group(k)\n",
    "            template_df = template_df.drop(columns=['F', 'Cell'])\n",
    "        cell_id = int(''.join(filter(str.isdigit, k)))\n",
    "        template_df[cell_id] = data_group.get_group(k)['F'].values\n",
    "        \n",
    "    \n",
    "    # Calculate Z-scores for each neuron column\n",
    "    neuron_columns = [col for col in template_df.columns if isinstance(col, int)]\n",
    "    for col in neuron_columns:\n",
    "        mean = template_df[col].mean()\n",
    "        std = template_df[col].std()\n",
    "        template_df[col] = (template_df[col] - mean) / std  # Replace with Z-scores\n",
    "    return template_df\n",
    "\n",
    "\n",
    "# Function to batch-load and process files\n",
    "def process_files_with_zscore(data_dir, file_list):\n",
    "    \"\"\"\n",
    "    Batch load and process files to add mouse number, condition, and calculate Z-scores.\n",
    "    \"\"\"\n",
    "    processed_data = []\n",
    "\n",
    "    for file_name in file_list:\n",
    "        file_path = os.path.join(data_dir, file_name)\n",
    "        \n",
    "        # Extract mouse number and condition from file name\n",
    "        parts = file_name.split('_')\n",
    "        mouse_number = parts[0]\n",
    "        condition = parts[1]\n",
    "        #print(parts[1])\n",
    "        \n",
    "        # Load the file\n",
    "        data = pd.read_csv(file_path)\n",
    "        \n",
    "        # Restructure the data with Z-scores\n",
    "        restructured_data = restructure_data_with_zscore(data)\n",
    "        \n",
    "        # Add mouse number and condition as columns\n",
    "        restructured_data['Mouse'] = mouse_number\n",
    "        restructured_data['Condition'] = condition\n",
    "        \n",
    "        # Append to the list\n",
    "        processed_data.append(restructured_data)\n",
    "    \n",
    "    # Concatenate all processed data into a single DataFrame\n",
    "    merged_data = pd.concat(processed_data, ignore_index=True)\n",
    "    return merged_data\n",
    "\n",
    "# Define directory and file list\n",
    "\n",
    "# Process files and create the merged DataFrame with Z-scores\n",
    "merged_data_with_zscore = process_files_with_zscore(data_dir, file_list)\n",
    "\n",
    "# Save the merged data with Z-scores to a new CSV file (optional)\n",
    "output_path = os.path.join('C:/Users/ding/Documents/github/CeA', 'material_merged_data_with_zscore.csv')\n",
    "merged_data_with_zscore.to_csv(output_path, index=False)\n",
    "\n",
    "print(\"Data restructuring with Z-scores completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data_with_zscore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [8], line 45\u001b[0m\n\u001b[0;32m     29\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m df\n\u001b[0;32m     31\u001b[0m \u001b[38;5;66;03m# Example usage:\u001b[39;00m\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# Assume 'merged_data_with_zscore' is the restructured data with Z-scores calculated.\u001b[39;00m\n\u001b[0;32m     33\u001b[0m \u001b[38;5;66;03m# The 'bitepoint' column is used to mark events.\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     43\u001b[0m \n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Apply the event index assignment\u001b[39;00m\n\u001b[1;32m---> 45\u001b[0m merged_data_with_zscore \u001b[38;5;241m=\u001b[39m assign_event_index_per_group(\u001b[43mmerged_data_with_zscore\u001b[49m, event_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbitepoint\u001b[39m\u001b[38;5;124m'\u001b[39m, group_columns\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMouse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCondition\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m     47\u001b[0m \u001b[38;5;66;03m# Check the output\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_data_with_zscore' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def assign_event_index_per_group(df, event_column='bitepoint', group_columns=['Mouse', 'Condition']):\n",
    "    \"\"\"\n",
    "    Assigns an incremental event index starting from 1 for each group of mouse and condition.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): The dataset containing time series data.\n",
    "        event_column (str): The column indicating events (e.g., 1 for event occurrence).\n",
    "        group_columns (list): Columns to group by (e.g., Mouse, Condition).\n",
    "        \n",
    "    Returns:\n",
    "        pd.DataFrame: DataFrame with an added 'Event_Index' column.\n",
    "    \"\"\"\n",
    "    # Initialize the 'Event_Index' column as NaN\n",
    "    df['Event_Index'] = None\n",
    "\n",
    "    # Group by Mouse and Condition\n",
    "    for _, group_data in df.groupby(group_columns):\n",
    "        # Find the indices where the event occurs (event_column == 1)\n",
    "        event_indices = group_data[group_data[event_column] == 1].index\n",
    "        #print(event_indices)\n",
    "        # Assign event index starting from 1 for each event occurrence in the group\n",
    "        df.loc[event_indices, 'Event_Index'] = range(1, len(event_indices) + 1)\n",
    "        \n",
    "        # Forward fill Event_Index to ensure all rows within the same group have the same Event_Index\n",
    "        #df['Event_Index'] = df['Event_Index'].ffill()\n",
    "\n",
    "    return df\n",
    "\n",
    "# Example usage:\n",
    "# Assume 'merged_data_with_zscore' is the restructured data with Z-scores calculated.\n",
    "# The 'bitepoint' column is used to mark events.\n",
    "# The dataframe should contain 'Mouse' and 'Condition' columns, as well as an event marker column (e.g., 'bitepoint').\n",
    "\n",
    "# Sample Data (make sure to replace this with your actual data):\n",
    "# merged_data_with_zscore = pd.DataFrame({\n",
    "#     'Mouse': ['Mouse1', 'Mouse1', 'Mouse2', 'Mouse2'],\n",
    "#     'Condition': ['Condition1', 'Condition1', 'Condition2', 'Condition2'],\n",
    "#     'bitepoint': [0, 1, 0, 1],\n",
    "#     'Other_Column': [0.5, 0.7, 0.6, 0.8]\n",
    "# })\n",
    "\n",
    "# Apply the event index assignment\n",
    "merged_data_with_zscore = assign_event_index_per_group(merged_data_with_zscore, event_column='bitepoint', group_columns=['Mouse', 'Condition'])\n",
    "\n",
    "# Check the output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'merged_data_with_zscore' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [9], line 57\u001b[0m\n\u001b[0;32m     53\u001b[0m pre_event_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m     54\u001b[0m post_event_time \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m30\u001b[39m\n\u001b[0;32m     56\u001b[0m event_windows \u001b[38;5;241m=\u001b[39m extract_event_windows(\n\u001b[1;32m---> 57\u001b[0m     data\u001b[38;5;241m=\u001b[39m\u001b[43mmerged_data_with_zscore\u001b[49m,\n\u001b[0;32m     58\u001b[0m     event_column\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbitepoint\u001b[39m\u001b[38;5;124m'\u001b[39m,  \u001b[38;5;66;03m# Adjust as per your event column\u001b[39;00m\n\u001b[0;32m     59\u001b[0m     pre_event_time\u001b[38;5;241m=\u001b[39mpre_event_time,\n\u001b[0;32m     60\u001b[0m     post_event_time\u001b[38;5;241m=\u001b[39mpost_event_time\n\u001b[0;32m     61\u001b[0m )\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# Save the extracted data to a CSV file (optional)\u001b[39;00m\n\u001b[0;32m     64\u001b[0m event_windows\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmaterial_event_windows_data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'merged_data_with_zscore' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "\n",
    "def extract_event_windows(data, event_column='bitepoint', pre_event_time=30, post_event_time=30):\n",
    "    \"\"\"\n",
    "    Extracts pre-event and post-event data around specified events from the dataset.\n",
    "    \n",
    "    Parameters:\n",
    "        data (pd.DataFrame): The restructured dataset with time series data.\n",
    "        event_column (str): The column indicating events (e.g., 1 for event occurrence).\n",
    "        pre_event_time (int): Number of time points before the event to include.\n",
    "        post_event_time (int): Number of time points after the event to include.\n",
    "    \n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the extracted data with additional metadata.\n",
    "    \"\"\"\n",
    "    # Ensure the event column exists\n",
    "    if event_column not in data.columns:\n",
    "        raise ValueError(f\"The specified event column '{event_column}' does not exist in the data.\")\n",
    "    \n",
    "    # Find all event indices\n",
    "    event_indices = data[data[event_column] == 1].index\n",
    "\n",
    "    # Initialize a list to store results\n",
    "    event_data = []\n",
    "\n",
    "    for event_index in event_indices:\n",
    "        # Define the window around the event\n",
    "        start_time = event_index - pre_event_time\n",
    "        end_time = event_index + post_event_time\n",
    "\n",
    "        # Ensure the window is within bounds\n",
    "        if start_time < 0 or end_time >= len(data):\n",
    "            continue\n",
    "\n",
    "        # Extract data for the time window\n",
    "        window_data = data.iloc[start_time:end_time + 1]  # Use .iloc to slice by row index\n",
    "\n",
    "        # Add metadata for the event\n",
    "        window_data = window_data.copy()\n",
    "        window_data['Time_Relative_to_Event'] = range(-pre_event_time, post_event_time + 1)\n",
    "        # Append to the list\n",
    "        event_data.append(window_data)\n",
    "\n",
    "    # Combine all event windows into a single DataFrame\n",
    "    event_windows_df = pd.concat(event_data, ignore_index=True)\n",
    "    return event_windows_df\n",
    "\n",
    "# Example usage:\n",
    "\n",
    "\n",
    "# Step 2: Extract event windows for each event (e.g., lick events)\n",
    "pre_event_time = 30\n",
    "post_event_time = 30\n",
    "\n",
    "event_windows = extract_event_windows(\n",
    "    data=merged_data_with_zscore,\n",
    "    event_column='bitepoint',  # Adjust as per your event column\n",
    "    pre_event_time=pre_event_time,\n",
    "    post_event_time=post_event_time\n",
    ")\n",
    "\n",
    "# Save the extracted data to a CSV file (optional)\n",
    "event_windows.to_csv('material_event_windows_data.csv', index=False)\n",
    "\n",
    "print(\"Pre-event and post-event data extraction completed!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'event_windows' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [10], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mevent_windows\u001b[49m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactive_neuron\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m\n\u001b[0;32m      2\u001b[0m results \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, group_data \u001b[38;5;129;01min\u001b[39;00m event_windows\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mMouse\u001b[39m\u001b[38;5;124m\"\u001b[39m,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCondition\u001b[39m\u001b[38;5;124m\"\u001b[39m]):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;66;03m# Find the indices where the event occurs (event_column == 1)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m     \u001b[38;5;66;03m#event_indices = group_data[group_data[event_column] == 1].index\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;66;03m#print(event_indices)\u001b[39;00m\n\u001b[0;32m      7\u001b[0m     \u001b[38;5;66;03m# Assign event index starting from 1 for each event occurrence in the group\u001b[39;00m\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;66;03m#df.loc[event_indices, 'Event_Index'] = range(1, len(event_indices) + 1)\u001b[39;00m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'event_windows' is not defined"
     ]
    }
   ],
   "source": [
    "event_windows['active_neuron']=0\n",
    "results = []\n",
    "for name, group_data in event_windows.groupby([\"Mouse\",\"Condition\"]):\n",
    "    # Find the indices where the event occurs (event_column == 1)\n",
    "    #event_indices = group_data[group_data[event_column] == 1].index\n",
    "    #print(event_indices)\n",
    "    # Assign event index starting from 1 for each event occurrence in the group\n",
    "    #df.loc[event_indices, 'Event_Index'] = range(1, len(event_indices) + 1)\n",
    "    filtered_df = group_data[(group_data['Time_Relative_to_Event'] >= 0) & (group_data['Time_Relative_to_Event'] <= 30)]\n",
    "    numeric_name_float_columns = filtered_df.loc[:, filtered_df.columns[filtered_df.columns.str.isnumeric().isna()]]\n",
    "    not_all_nan_columns = numeric_name_float_columns.columns[~numeric_name_float_columns.isna().all()]\n",
    "    #no_nan_columns = numeric_name_float_columns.columns[numeric_name_float_columns.notna().all()]\n",
    "    selected_df=numeric_name_float_columns.loc[:,not_all_nan_columns]\n",
    "\n",
    "    chunk_size = 31\n",
    "\n",
    "    # Initialize a list to store the results\n",
    "\n",
    "# Iterate over the DataFrame in chunks of 31 rows\n",
    "    for start in range(0, len(selected_df), chunk_size):\n",
    "        end = start + chunk_size\n",
    "        chunk = selected_df.iloc[start:end]  # Get the current chunk\n",
    "        #max_values = chunk.max()  # Find the max value in the chunk\n",
    "        mean_values = chunk.mean() \n",
    "        #num_columns_gt_0_5 = (max_values > 1).sum()  # Count columns > 0.5\n",
    "        num_columns_gt_0_5 = (mean_values > 0.5).sum()\n",
    "        total_columns = len(selected_df.columns)  # Total number of columns\n",
    "        results.append((num_columns_gt_0_5, total_columns,name))  # Store the result\n",
    "\n",
    "\n",
    "results_df = pd.DataFrame(results, columns=['active_number', 'total_number', 'group'])\n",
    "    # Forward fill Event_Index to ensure all rows within the same group have the same Event_Index\n",
    "    #df['Event_Index'] = df['Event_Index'].ffill()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df.to_csv('C:/Users/ding/Documents/github/CeA/material_active_neuron_number_0.3.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'results_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [11], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mseaborn\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01msns\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Load the data\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#file_path = 'path_to_your_file.csv'  # Update with your file path if needed\u001b[39;00m\n\u001b[1;32m----> 7\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43mresults_df\u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Calculate the percentage of active neurons\u001b[39;00m\n\u001b[0;32m     10\u001b[0m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactive_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m (data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactive_number\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m/\u001b[39m data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtotal_number\u001b[39m\u001b[38;5;124m'\u001b[39m]) \u001b[38;5;241m*\u001b[39m \u001b[38;5;241m100\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'results_df' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "#file_path = 'path_to_your_file.csv'  # Update with your file path if needed\n",
    "data = results_df\n",
    "\n",
    "# Calculate the percentage of active neurons\n",
    "data['active_percentage'] = (data['active_number'] / data['total_number']) * 100\n",
    "\n",
    "# Assign a unique event index per mouse and condition\n",
    "data['event_index'] = data.groupby('group').cumcount() + 1\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "# Example to ensure the 'group' column is a string\n",
    "data['group'] = data['group'].astype(str)\n",
    "\n",
    "# Use regular expression to extract 'mouse' and 'condition'\n",
    "data[['mouse', 'condition']] = data['group'].str.extract(r\"\\('(\\d+)',\\s*'(\\w+)'\\)\")\n",
    "\n",
    "# Convert 'mouse' to integer if necessary\n",
    "data['mouse'] = data['mouse'].astype(int)  # Converts 'mouse' to integers\n",
    "\n",
    "# Preview the result\n",
    "print(data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn [12], line 7\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Group by 'mouse' and 'condition' to calculate average percentage for each mouse within a condition\u001b[39;00m\n\u001b[0;32m      6\u001b[0m mouse_condition_summary \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m----> 7\u001b[0m     \u001b[43mdata\u001b[49m\u001b[38;5;241m.\u001b[39mgroupby([\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmouse\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m'\u001b[39m])\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;241m.\u001b[39magg(mean_active\u001b[38;5;241m=\u001b[39m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mactive_percentage\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean\u001b[39m\u001b[38;5;124m'\u001b[39m))\n\u001b[0;32m      9\u001b[0m     \u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     12\u001b[0m \u001b[38;5;66;03m# Sort conditions by the mean active percentage from biggest to smallest\u001b[39;00m\n\u001b[0;32m     13\u001b[0m condition_order \u001b[38;5;241m=\u001b[39m mouse_condition_summary\u001b[38;5;241m.\u001b[39mgroupby(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcondition\u001b[39m\u001b[38;5;124m'\u001b[39m)[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmean_active\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmean()\u001b[38;5;241m.\u001b[39msort_values(ascending\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\u001b[38;5;241m.\u001b[39mindex\n",
      "\u001b[1;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Group by 'mouse' and 'condition' to calculate average percentage for each mouse within a condition\n",
    "mouse_condition_summary = (\n",
    "    data.groupby(['mouse', 'condition'])\n",
    "    .agg(mean_active=('active_percentage', 'mean'))\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "# Sort conditions by the mean active percentage from biggest to smallest\n",
    "condition_order = mouse_condition_summary.groupby('condition')['mean_active'].mean().sort_values(ascending=False).index\n",
    "\n",
    "# Plot box plot for each condition with the conditions sorted by the mean active percentage\n",
    "plt.figure(figsize=(8, 6))\n",
    "sns.boxplot(\n",
    "    x='condition',\n",
    "    y='mean_active',\n",
    "    data=mouse_condition_summary,\n",
    "    order=condition_order,  # Sort conditions based on the mean active percentage\n",
    "    palette='muted'\n",
    ")\n",
    "\n",
    "# Add individual data points on top of the box plot using stripplot\n",
    "sns.stripplot(\n",
    "    x='condition',\n",
    "    y='mean_active',\n",
    "    data=mouse_condition_summary,\n",
    "    order=condition_order,\n",
    "    color='black',  # Color for individual points\n",
    "    jitter=True,    # Adds random noise to the x-position to avoid overlap\n",
    "    alpha=0.5,      # Transparency for better visibility of overlapping points\n",
    "    size=6          # Size of the points\n",
    ")\n",
    "\n",
    "# Customize the plot\n",
    "plt.xlabel(\"Condition\")\n",
    "plt.ylabel(\"Percentage of Active Neurons (%)\")\n",
    "plt.title(\"Distribution of Active Neurons by Condition\")\n",
    "plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "plt.tight_layout()\n",
    "\n",
    "# Show the plot\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "local-venv",
   "language": "python",
   "name": "local-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
